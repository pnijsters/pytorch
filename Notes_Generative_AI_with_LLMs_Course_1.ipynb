{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPWR3b629+5GT0eqE2lnVLT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pnijsters/pytorch/blob/main/Notes_Generative_AI_with_LLMs_Course_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Large Language Models\n",
        "\n",
        "Foundation models (in order of parameter size)\n",
        "1. GPT - decoder only model\n",
        "2. Bloom (176B) - decoder only model\n",
        "3. FLAN-T5 - encoder and decoder model\n",
        "4. PaLM\n",
        "5. LLaMa - decoder only model\n",
        "6. BERT (110M) - encoder only model\n",
        "\n",
        "Parameters = models memory\n",
        "\n",
        "Prompt = input to the model\n",
        "\n",
        "Context window = the size of the maximum input to the model, typically a couple of thousand words\n",
        "\n",
        "Completion = output of a model\n",
        "\n",
        "\n",
        "# Generative AI\n",
        "\n",
        "GenAI is not new, it started with RNNs: Recurrent Neural Networks. The Transformers Architecture took it to a whole new level in 2017. RNNs are very compute resource intensive: doesnt scale well enough.\n",
        "\n",
        "Transformers Architecture: Attention Is All You Need. Research paper from Google and University of Torronto. This unlocked the progress recently made because of the following capabilities:\n",
        "\n",
        "- scales efficiently\n",
        "- parallel process\n",
        "- attention to input meaning\n",
        "\n",
        "Three things that enabled the recent progress in the ever larger LLMs are:\n",
        "\n",
        "- Transformer architecture\n",
        "- Availability of massive training data sets\n",
        "- More powerful compute resources (GPUs)\n",
        "- Massive amounts of CAPEX inflow in the hype cycle\n",
        "\n",
        "Paper is posted here: https://arxiv.org/abs/1706.03762\n",
        "\n",
        "## Tokenizer: translating text into numbers\n",
        "\n",
        "Embedding is 'translating' text into numbers. This is done by assigning each word or parts of words in the input text a number which is basically an index into a long list of all the words the model supports. This function is done during embedding and uses a \"tokenizer\". The number assigned is called a \"token ID\".\n",
        "\n",
        "*Tokens* are words, character sets, or combinations of words and punctuation that are used by large language models (LLMs) to decompose text into. Tokens are the representations of text in the form of a vector.\n",
        "\n",
        "## Embedding\n",
        "\n",
        "Vague babble: TBD\n",
        "\n",
        "A 'token ID' is mapped to a vector.\n",
        "\n",
        "## Encoder\n",
        "The encoder inputs (\"prompts\") with contextual understanding and produces one vector per input token\n",
        "\n",
        "## Decoder\n",
        "Accepts input tokens and generates new tokens\n",
        "\n",
        "# Prompt engineering\n",
        "\n",
        "- In-context learning (ICL) is providing examples in the prompt.\n",
        "- Zero-shot inference is when you provide the input data in the prompt\n",
        "- Single-shot inference is when you provide an example in the input data in the prompt\n",
        "- Few shot inference is when you add multiple examples in the input data in the prompt\n",
        "\n",
        "Larger models tend to work well with zero-shot, the smaller models typically need single or few shot inference for the output to make sense.\n",
        "\n",
        "# Generative configuration\n",
        "\n",
        "- Max new tokens\n",
        "\n",
        "Greedy decoding or greedy sampling means that the model will always chose the word in the dictionary with the highest calculated probability. Random(-weighted) sampling selects a token/word based on a random-weighted strategy across all tokens which results in a more creative, less repetitive output. Top K and Top P are methods to limit the randomness and avoid completely bizarre outputs\n",
        "\n",
        "- **Sample top K**: limit the model to only chose from the top K options instead of the entire dictionary\n",
        "- **Sample top P**: limit the model to only chose from options where the cumulative probability of all these options is smaller than P\n",
        "- **Temperature**: influences the shape of the probability distribution the model uses to select a token. The higher the temperature > the higher the randomness\n",
        "\n",
        "# Training LLMs\n",
        "\n",
        "**Encoder only models or autoencoding models** are trained using Masked Language Modeling (MLM). MLM takes a sentence and removes or 'masks' a random word and lets the model predict that word. The objective is to reconstruct the original text which is called 'denoising'. The models look at the sentence bidirectionally: from left to right and right to left to take in the entire context of the sentence before predicting the word. Good usecases for these models:\n",
        "\n",
        "- Sentiment analysis\n",
        "- Named entity recognition\n",
        "- Word classification\n",
        "- Example models: BERT and ROBERTA\n",
        "\n",
        "**Decoder only models or autoregressive models** are trained using Causal Language Modeling (CLM). The objective is to predict the next token based on the previous sequence of tokens. The model only works from left to right and up to the tokens that are not masked. These models do not have knowledge or context of tokens beyond the masked token. Good usecases for these models:\n",
        "\n",
        "- Text generation\n",
        "- Other emergent behavior (?) depends on size of model\n",
        "- Example models: GPT and BLOOM\n",
        "\n",
        "** Decoder and encoder models or sequence-to-sequence models** are trained using Span Corruption where multiple tokens are masked and replaced with a 'sentinel token' that represents these multiple masked tokens. Exact details vary model to model. Good usecases for these models:\n",
        "\n",
        "- Translation\n",
        "- Text summarization\n",
        "- Question answering\n",
        "- Example models: T5 and BART\n",
        "\n",
        "## Quantization\n",
        "\n",
        "Quantization is reducing the required memory to store and train models. Quantization-aware training (QAT) learns the quantization scaling factors during training.\n",
        "\n",
        "Training a model of 1B parameters requires the following memory:\n",
        "\n",
        "- 4 bytes for each parameter at 32-bit precission\n",
        "- 8 bytes for 2 states in the Adam optimizer\n",
        "- 4 bytes for the gradient\n",
        "- 8 bytes for activations and temporary memory\n",
        "\n",
        "And thus, each parameter requires 24 bytes of memory. A 1B model therefor requires a GPU with 24GB of memory.\n",
        "\n",
        "Quantization is remapping 32-bit (FP32) to 16-bit (FP16) floating point or 8-bit integer (INT8). BFLOAT16 or BFP16 or Brain Floating Point 16 was developed by Google Brain team and is popular with LLMs and is now supported by NVIDIA GPUs. BFP16 is also known as 'Truncated FP32' since the precision bits (fraction bits) are just truncated while keeping the total range.\n",
        "\n",
        "- **FP32**: 1 sign bit, 8 exponent bits, 23 fraction bits. Range: -3e38 to 3e38\n",
        "- **FP16**: 1 sign bit, 5 exponent bits, 10 fraction bits. Range: -65k to 65k\n",
        "- **BFP16**: 1 sign bit, 8 exponent bits, 7 fraction bits. Range: -3e38 to 3e38\n",
        "- **INT8**: 1 sign bit, 0 exponent bits, 7 fraction bits. Range -128 to 127\n",
        "\n",
        "FLAN-T5 uses BFP16.\n",
        "\n",
        "A 500B parameter model requires 12,000GB @ 32-bit full precision (!). Cannot be done on a single GPU: you need multiple GPUs in parallel (sometime 100s!)\n",
        "\n",
        "PyTorch: Distributed Data Parallel (DDP)\n",
        "PyTorch: Fully Sharded Data Parallel (FSDP) > ZeRO\n",
        "\n",
        "## Compute performance\n",
        "\n",
        "1 \"petaflops/s-day\" = floating point operations performed at a rate of 1 petaFLOP per second for one day. 1 petaFLOP = 1,000,000,000,000,000 (one quadrillion) floating point operations per second.\n",
        "\n",
        "1 PetaFLOP/s-day = 8 x NVIDIA V100 running at full efficiency = 2 x NVIDIA A100\n",
        "\n",
        "GPT-3-175B requires 5000 Petaflops/s-day for training:\n",
        "- 5,000 days of running 1 Petaflops/s-day: 2 x A100\n",
        "- 50 days of running 100 Petaflops/s-day: 200 x A100\n",
        "- 5 days of running 1,000 Petaflops/s-day: 2,000 x A100\n",
        "- 0.5 days or running 10,000 Petaflops/s-day: 20,000 x A100\n",
        "\n",
        "\n",
        "Iron triangle:\n",
        "- data set size\n",
        "- model size\n",
        "- compute budget\n",
        "\n",
        "Chincilla scaling laws dictate that the ideal data set size is 20 times the model size. Meaning: 50B parameter model needs 50B x 20 input data set size. THe reality is that a lot of the very large parameter models are *undertrained* because they have not been fed enough training data.\n",
        "\n",
        "# Fine tuning\n",
        "\n",
        "Fine tuning is a supervised learning process whereby de model is fed with labeled examples to update the weights of the model (prompt-completion pairs).\n",
        "\n",
        "Instruction fine tuning is like prompt engineering with one or more examples on how the model is to behave and perform. Full fine-tuning is a fine tuning process whereby all of the models parameters are updated; it results in a new version of the model with updated weights. This approach requires the full resource footprint of the original training exercise.\n",
        "\n",
        "There are templates and tools available that can be used to feed the LLMs with these fine tuning prompts. Only 500-1000 examples can start to yield decent results.\n",
        "\n",
        "One of the downsides of full fine-tuning is that you get a completely new version of the original LLM for every specific fine-tuned task. Which can be prohibitive since every LLM is typically many GBs in size. Using parameter efficient fine-tuning if possible addresses this downside.\n",
        "\n",
        "## FLAN models\n",
        "\n",
        "FLAN = Fine-tuned Language Net. A specific set of instructions was used to fine-tune these models. FLAN-T5 is the FLAN instructed version of T5 base model. The FLAN paper is posted here: https://arxiv.org/abs/2210.11416\n",
        "\n",
        "\n",
        "## Catastrophic forgetting\n",
        "Fine-tuning can significantly increase the performance of a model on a specific task but at the same time have sever negative impact on other tasks because the weights of the model are being updated. Depending on the use case of the focused task, catastrophic forgetting is not an issue.\n",
        "\n",
        "Otherwise, do fine-tuning on multiple tasks at the same time with larger data sets. Or consider using **Parameter Efficient Fine Tuning (PEFT)**.\n",
        "\n",
        "# Model evaluation metrics\n",
        "\n",
        "Accuracy = correct predictions / total predicitions.\n",
        "\n",
        "This works well for traditional machine learning with supervised data sets where the output is known and can be compared to the predicted value. For LLMs this is not the case since output isnt known...\n",
        "\n",
        "\"Mike really loves drinking tea\" = \"Mike adores sipping tea\"\n",
        "\"Mike does not drink coffee\" != \"Mike does drink coffee\"\n",
        "\n",
        "Both examples are very similar in the differences between both responses are small. However the coffee example is clearly wrong whereas the tea example is not.\n",
        "\n",
        "## Evaluation metrics for LLMs\n",
        "\n",
        "- Rouge: Used for text summarization. Copares a summary to one or more reference summaries\n",
        "- Blue (Bilingual Evaluation Understudy) score: Used for text translation. Compares to human generated translations\n",
        "\n",
        "These are both fairly simple evaluation methods to use for initial evaluation and evaluation during fine-tuning. For better insights it is better to use common Benchmarks:\n",
        "\n",
        "- Glue: General Purpose Language Understanding (2018)\n",
        "- SuperGlue\n",
        "- Helm: Holistic Evaluation of Language Models\n",
        "- BIG-Bench (2022)\n",
        "- MMLU: Massive Multitask Language Understanding (2021)\n",
        "\n",
        "\n",
        "# Parameter Efficient Fine-Tuning (PEFT)\n",
        "\n",
        "Fine-tuning the entire parameter space on LLMs might be cost prohibitive. More efficient tuning can be done be either:\n",
        "\n",
        "- Frozen Weights: fix a large part of the parameter space and only allow changes to a subset of the parameters (15-20% of total LLM weights)\n",
        "- Additional Layers: add additional layers to the model and keep the parameters of the original layers fixed, only train the weights in the new layers\n",
        "\n",
        "PEFT is less prone to catastrophic forgetting because only a small number of weights get updated.\n",
        "\n",
        "PEFT is also more efficient for model storage. Each LLM typically is many GBs in size. With PEFT only the delta of the new trained parameters needs to be stored (typically order of MBs). These PEFT weight are combined with the original LLM weights at inference.\n",
        "\n",
        "PEFT methods:\n",
        "\n",
        "- Select subset of initial parameters to fine-tune\n",
        "- Reparameterization model weights using a low-rank representation (LoRA)\n",
        "- Add trainable layers or parameters to the model\n",
        "    - Adapters\n",
        "    - Soft Prompts: Prompt Tuning\n",
        "\n",
        "## LoRA - Low Rank Adaptation\n",
        "\n",
        "- Freeze most of the original LLM weights\n",
        "- Inject 2 rank decomposition matrices\n",
        "- Train the weigths of the smaller matrices\n",
        "\n",
        "## Prompt tuning\n",
        "\n",
        "Prompt tuning != prompt engineering.\n",
        "\n",
        "Prompt tuning adds trainable 'soft prompts' to the input; prepends these to the prompt: typically 20-100 tokens. These virtual tokens do not need to map to specific words in the embedding space.\n",
        "\n",
        "\n",
        "# RLHF - Reinforcement Learning from Human Feedback\n",
        "\n",
        "Models behaving badly:\n",
        "- toxix language\n",
        "- aggresssive responses\n",
        "- providing dangerous information\n",
        "\n",
        "HHH principles, completions should be:\n",
        "- Helpful\n",
        "- Honest\n",
        "- Harmless\n",
        "\n",
        "## RAG - Retrieval Augmented Generation\n",
        "\n",
        "Lets LLM access external data sources at inference time\n",
        "\n",
        "## PAL - Program Aided Language Model\n",
        "\n",
        "Pairs an LLM with an external code interpreter to perform calculations (typically Python)\n",
        "\n",
        "## ReACT - Reasoning and Action\n",
        "Paper here: https://arxiv.org/abs/2210.03629\n",
        "\n",
        "## LangChain\n",
        "\n",
        "The orchestration framework for RAG, PAL and ReACT\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WvAjDplRd3Se"
      }
    }
  ]
}